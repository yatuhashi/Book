#ゼロから作るDeepLearning

##1章 Pythonの使い方
```
def HowToPython():
    #前略
    pass
```

##2章 パーセプトロン
原始的なニューロン遷移の考え方
##3章 ニューラルネットワーク説明・推論
1. 論理回路の話
2. 活性化関数の話
    - シグモイド関数
    - ReLU関数
###他にはないのか？
3. ニューラルネットワークの計算  
    * 入力:x 重み: w 出力: yとして、行列積で求められる  
4. ニューロンでの出力の話
入力して計算したものをh(a)にいれた物が出力結果yになる
h()は、活性化関数
5. 最後の出力層の話
    * 恒等関数とソフトマックス関数がある  
    * ソフトマックス関数はそのまま確率になり、出力全部を足すと1  
    * 出力する次元は、ものによって変わる  
    * 例えば、10種類の数字を推論しようとしているのであれば10個の出力  
    * 同時に１００枚とかを推論できる  
    * 例  
       - 28*28=784の画像を推論するとする  
       - 入力：100*784  
       - 出力：100*10  
       - になる  

##4章 ニューラルネットワーク学習
   * 勾配法を使う  
####他にはどんなのがあるのか？
   * 今までの機械学習では学習させる前に特徴量を抽出し、それを学習させてた
        - 特徴量
            - SIFT
            - SURF
            - HOG
        - 識別器
            - SVM
            - KNN
   * ニューラルネットワークでは画像をそのまま学習し、特徴量も学習する
   * あるデータセットだけに過度に対応した状態を過学習という
   * ニューラルネットワークで学習するとは最適な重みを見つける事
   * 学習する指標を損失関数という
   * 損失関数例
        - 二乗和誤差
        - 交差エントロピー誤差
   * 勾配降下法
        - 結果を微分し、損失関数が最小になるようにに少しずつパラメーターを変えていく
   * ミニバッチ法
        - 多くの学習データから少しを取り出し、学習する方法
   * 1epoch
        - ミニバッチで学習データ一回分の量をやった分
   * epoch毎にテストデータから認識率を導出し、過学習していないか確認する

##5章 誤差逆伝播法

##6章 学習に関するテクニック

##7章 畳み込みニューラルネットワーク

##8章 ディープラーニング

